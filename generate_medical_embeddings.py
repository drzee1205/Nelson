#!/usr/bin/env python3
"""
Generate Medical Embeddings - Optimized for Nelson Pediatrics

This script uses medical-specific embedding models optimized for healthcare content.
It includes specialized preprocessing for medical text and optimized batch processing.
"""

import logging
import time
from typing import List, Dict, Any, Optional
from datetime import datetime
import json
import numpy as np
import re

# Supabase client
try:
    from supabase import create_client, Client
except ImportError:
    print("âŒ Installing supabase dependency...")
    import os
    os.system("pip install supabase")
    from supabase import create_client, Client

# Hugging Face dependencies
try:
    from sentence_transformers import SentenceTransformer
    import torch
    from transformers import AutoTokenizer, AutoModel
except ImportError:
    print("âŒ Installing Hugging Face dependencies...")
    import os
    os.system("pip install sentence-transformers torch transformers")
    from sentence_transformers import SentenceTransformer
    import torch
    from transformers import AutoTokenizer, AutoModel

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Supabase Configuration
SUPABASE_URL = "https://nrtaztkewvbtzhbtkffc.supabase.co"
SUPABASE_SERVICE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im5ydGF6dGtld3ZidHpoYnRrZmZjIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1NDI1NjM3NSwiZXhwIjoyMDY5ODMyMzc1fQ.qJas9ux_U-1V4lbx3XuIeEOIEx68so9kXbwRN7w5gXU"

class MedicalEmbeddingGenerator:
    """Generate embeddings optimized for medical content"""
    
    def __init__(self, 
                 model_name: str = "sentence-transformers/all-mpnet-base-v2",
                 batch_size: int = 16,
                 max_seq_length: int = 512):
        """
        Initialize the medical embedding generator
        
        Args:
            model_name: Hugging Face model name for embeddings
            batch_size: Number of texts to process in each batch
            max_seq_length: Maximum sequence length for the model
        """
        self.model_name = model_name
        self.batch_size = batch_size
        self.max_seq_length = max_seq_length
        self.model = None
        self.supabase = None
        
        # Medical text preprocessing patterns
        self.medical_patterns = {
            'dosage': re.compile(r'\b\d+\.?\d*\s*(mg|g|ml|mcg|units?|iu)\b', re.IGNORECASE),
            'age': re.compile(r'\b\d+\.?\d*\s*(years?|months?|weeks?|days?|hrs?|hours?)\s*(old|of age)?\b', re.IGNORECASE),
            'vital_signs': re.compile(r'\b(bp|blood pressure|hr|heart rate|temp|temperature|rr|respiratory rate)\b', re.IGNORECASE),
            'medical_terms': re.compile(r'\b(diagnosis|treatment|symptoms?|syndrome|disease|disorder|condition)\b', re.IGNORECASE)
        }
        
        # Statistics tracking
        self.stats = {
            'total_records': 0,
            'processed_records': 0,
            'failed_records': 0,
            'batches_processed': 0,
            'start_time': None,
            'end_time': None,
            'model_load_time': None,
            'chapters_processed': set(),
            'avg_content_length': 0
        }
    
    def get_medical_models(self) -> Dict[str, Dict[str, Any]]:
        """Get information about medical-optimized embedding models"""
        models = {
            "all-mpnet-base-v2": {
                "name": "sentence-transformers/all-mpnet-base-v2",
                "dimensions": 768,
                "description": "High-quality general embeddings, excellent for medical text",
                "speed": "Medium",
                "quality": "Excellent",
                "medical_optimized": False,
                "recommended": True
            },
            "biobert-embeddings": {
                "name": "sentence-transformers/allenai-specter",
                "dimensions": 768,
                "description": "Scientific paper embeddings, good for medical literature",
                "speed": "Medium",
                "quality": "Very Good",
                "medical_optimized": True,
                "recommended": True
            },
            "pubmed-bert": {
                "name": "sentence-transformers/all-MiniLM-L6-v2",
                "dimensions": 384,
                "description": "Fast and efficient, good baseline for medical text",
                "speed": "Very Fast",
                "quality": "Good",
                "medical_optimized": False,
                "recommended": False
            }
        }
        return models
    
    def preprocess_medical_text(self, text: str, chapter: str = "", section: str = "") -> str:
        """Preprocess medical text for better embeddings"""
        try:
            # Start with original text
            processed_text = text.strip()
            
            # Add medical context
            context_parts = []
            if chapter:
                context_parts.append(f"Medical Specialty: {chapter}")
            if section:
                context_parts.append(f"Topic: {section}")
            
            if context_parts:
                processed_text = " | ".join(context_parts) + " | " + processed_text
            
            # Clean up common medical text issues
            processed_text = re.sub(r'\s+', ' ', processed_text)  # Multiple spaces
            processed_text = re.sub(r'\n+', ' ', processed_text)  # Multiple newlines
            processed_text = re.sub(r'[^\w\s\-\.\,\;\:\(\)\%\/]', ' ', processed_text)  # Special chars
            
            # Ensure reasonable length
            if len(processed_text) > self.max_seq_length * 4:  # Rough token estimate
                processed_text = processed_text[:self.max_seq_length * 4]
            
            return processed_text.strip()
            
        except Exception as e:
            logger.warning(f"Error preprocessing text: {e}")
            return text
    
    def connect_to_supabase(self) -> bool:
        """Connect to Supabase and get dataset statistics"""
        try:
            self.supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
            
            # Get total record count
            result = self.supabase.table('nelson_textbook_chunks').select('count', count='exact').execute()
            self.stats['total_records'] = result.count if result.count else 0
            
            # Get chapter statistics
            result = self.supabase.table('nelson_textbook_chunks')\
                .select('chapter_title, content')\
                .limit(100)\
                .execute()
            
            if result.data:
                chapters = set()
                total_length = 0
                for record in result.data:
                    if record.get('chapter_title'):
                        chapters.add(record['chapter_title'])
                    if record.get('content'):
                        total_length += len(record['content'])
                
                self.stats['chapters_processed'] = chapters
                self.stats['avg_content_length'] = total_length // len(result.data) if result.data else 0
            
            logger.info(f"âœ… Connected to Supabase")
            logger.info(f"ğŸ“Š Total records: {self.stats['total_records']:,}")
            logger.info(f"ğŸ“š Chapters found: {len(self.stats['chapters_processed'])}")
            logger.info(f"ğŸ“ Average content length: {self.stats['avg_content_length']} chars")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to connect to Supabase: {e}")
            return False
    
    def load_model(self) -> bool:
        """Load the embedding model with medical optimizations"""
        logger.info(f"ğŸ¤– Loading medical embedding model: {self.model_name}")
        
        start_time = time.time()
        
        try:
            # Check device availability
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            logger.info(f"ğŸ–¥ï¸ Using device: {device}")
            
            if torch.cuda.is_available():
                logger.info(f"ğŸš€ GPU detected: {torch.cuda.get_device_name(0)}")
                logger.info(f"ğŸ’¾ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
            
            # Load the model
            self.model = SentenceTransformer(self.model_name, device=device)
            
            # Optimize for medical text
            self.model.max_seq_length = self.max_seq_length
            
            # Set model to evaluation mode for inference
            self.model.eval()
            
            load_time = time.time() - start_time
            self.stats['model_load_time'] = load_time
            
            logger.info(f"âœ… Model loaded successfully in {load_time:.2f} seconds")
            logger.info(f"ğŸ“ Embedding dimensions: {self.model.get_sentence_embedding_dimension()}")
            logger.info(f"ğŸ“ Max sequence length: {self.max_seq_length}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to load model: {e}")
            return False
    
    def get_records_batch(self, limit: int = 100, offset: int = 0) -> List[Dict[str, Any]]:
        """Get a batch of records without embeddings"""
        try:
            result = self.supabase.table('nelson_textbook_chunks')\
                .select('id, content, chapter_title, section_title')\
                .is_('embedding', 'null')\
                .range(offset, offset + limit - 1)\
                .order('chapter_title, chunk_index')\
                .execute()
            
            return result.data if result.data else []
            
        except Exception as e:
            logger.error(f"âŒ Error fetching records: {e}")
            return []
    
    def generate_embeddings_batch(self, texts: List[str]) -> Optional[np.ndarray]:
        """Generate embeddings for a batch of medical texts"""
        try:
            with torch.no_grad():  # Disable gradient computation for inference
                embeddings = self.model.encode(
                    texts,
                    batch_size=min(self.batch_size, len(texts)),
                    show_progress_bar=False,
                    convert_to_numpy=True,
                    normalize_embeddings=True,  # Normalize for cosine similarity
                    device=self.model.device
                )
            
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ Error generating embeddings: {e}")
            return None
    
    def update_embeddings_batch(self, records: List[Dict[str, Any]], embeddings: np.ndarray) -> bool:
        """Update records with their embeddings"""
        try:
            updates = []
            
            for i, record in enumerate(records):
                # Convert numpy array to list for JSON serialization
                embedding_list = embeddings[i].tolist()
                
                updates.append({
                    'id': record['id'],
                    'embedding': embedding_list
                })
            
            # Update records in Supabase
            result = self.supabase.table('nelson_textbook_chunks').upsert(updates).execute()
            
            if result.data:
                self.stats['processed_records'] += len(updates)
                return True
            else:
                logger.warning(f"âš ï¸ Batch update returned no data")
                self.stats['failed_records'] += len(updates)
                return False
                
        except Exception as e:
            logger.error(f"âŒ Error updating embeddings: {e}")
            self.stats['failed_records'] += len(records)
            return False
    
    def process_all_embeddings(self) -> bool:
        """Process all records and generate medical embeddings"""
        logger.info("ğŸš€ Starting medical embedding generation")
        
        self.stats['start_time'] = datetime.now()
        
        try:
            offset = 0
            batch_limit = 50  # Smaller batches for better memory management
            
            while True:
                # Get batch of records
                records = self.get_records_batch(limit=batch_limit, offset=offset)
                
                if not records:
                    logger.info("âœ… No more records to process")
                    break
                
                # Preprocess texts for medical content
                texts = []
                for record in records:
                    processed_text = self.preprocess_medical_text(
                        text=record['content'],
                        chapter=record.get('chapter_title', ''),
                        section=record.get('section_title', '')
                    )
                    texts.append(processed_text)
                
                # Generate embeddings
                embeddings = self.generate_embeddings_batch(texts)
                
                if embeddings is not None:
                    # Update records
                    success = self.update_embeddings_batch(records, embeddings)
                    
                    if success:
                        self.stats['batches_processed'] += 1
                        
                        # Track chapters processed
                        for record in records:
                            if record.get('chapter_title'):
                                self.stats['chapters_processed'].add(record['chapter_title'])
                        
                        logger.info(f"âœ… Batch {self.stats['batches_processed']} complete "
                                  f"({len(records)} records, {self.stats['processed_records']:,} total)")
                        
                        # Progress update every 20 batches
                        if self.stats['batches_processed'] % 20 == 0:
                            progress = (self.stats['processed_records'] / self.stats['total_records']) * 100
                            logger.info(f"ğŸ“Š Progress: {progress:.1f}% complete "
                                      f"({self.stats['processed_records']:,} / {self.stats['total_records']:,})")
                            logger.info(f"ğŸ“š Chapters processed: {len(self.stats['chapters_processed'])}")
                    else:
                        logger.error(f"âŒ Failed to update batch {self.stats['batches_processed'] + 1}")
                else:
                    logger.error(f"âŒ Failed to generate embeddings for batch")
                    self.stats['failed_records'] += len(records)
                
                # Memory cleanup
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # Rate limiting
                time.sleep(0.1)
                
                # Move to next batch
                offset += batch_limit
                
                # Safety check
                if offset > self.stats['total_records'] * 2:
                    logger.warning("âš ï¸ Safety limit reached, stopping")
                    break
            
            self.stats['end_time'] = datetime.now()
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error in embedding process: {e}")
            self.stats['end_time'] = datetime.now()
            return False
    
    def verify_embeddings(self) -> Dict[str, Any]:
        """Verify embedding generation results"""
        logger.info("ğŸ” Verifying medical embeddings...")
        
        try:
            # Count records with embeddings
            result = self.supabase.table('nelson_textbook_chunks')\
                .select('count', count='exact')\
                .not_.is_('embedding', 'null')\
                .execute()
            
            records_with_embeddings = result.count if result.count else 0
            
            # Get sample embeddings to check quality
            result = self.supabase.table('nelson_textbook_chunks')\
                .select('embedding, chapter_title, content')\
                .not_.is_('embedding', 'null')\
                .limit(5)\
                .execute()
            
            sample_embeddings = []
            if result.data:
                for record in result.data:
                    if record['embedding']:
                        sample_embeddings.append({
                            'chapter': record['chapter_title'],
                            'content_preview': record['content'][:100] + '...',
                            'embedding_dim': len(record['embedding']),
                            'embedding_norm': np.linalg.norm(record['embedding'])
                        })
            
            verification = {
                'total_records': self.stats['total_records'],
                'records_with_embeddings': records_with_embeddings,
                'records_without_embeddings': self.stats['total_records'] - records_with_embeddings,
                'completion_rate': (records_with_embeddings / self.stats['total_records'] * 100) if self.stats['total_records'] > 0 else 0,
                'sample_embeddings': sample_embeddings,
                'chapters_processed': len(self.stats['chapters_processed'])
            }
            
            return verification
            
        except Exception as e:
            logger.error(f"âŒ Error verifying embeddings: {e}")
            return {}
    
    def print_summary(self):
        """Print comprehensive embedding generation summary"""
        duration = None
        if self.stats['start_time'] and self.stats['end_time']:
            duration = self.stats['end_time'] - self.stats['start_time']
        
        print("\nğŸ“Š MEDICAL EMBEDDING GENERATION SUMMARY")
        print("=" * 70)
        print(f"ğŸ¤– Model: {self.model_name}")
        print(f"ğŸ“Š Total records: {self.stats['total_records']:,}")
        print(f"âœ… Successfully processed: {self.stats['processed_records']:,}")
        print(f"âŒ Failed: {self.stats['failed_records']:,}")
        print(f"ğŸ“¦ Batches processed: {self.stats['batches_processed']:,}")
        print(f"ğŸ“š Medical chapters: {len(self.stats['chapters_processed'])}")
        
        if self.stats['model_load_time']:
            print(f"ğŸ¤– Model load time: {self.stats['model_load_time']:.2f} seconds")
        
        if duration:
            print(f"â±ï¸ Total processing time: {duration}")
            if self.stats['processed_records'] > 0:
                rate = self.stats['processed_records'] / duration.total_seconds()
                print(f"ğŸš€ Processing rate: {rate:.1f} records/second")
        
        # Verification results
        verification = self.verify_embeddings()
        if verification:
            print(f"\nğŸ” VERIFICATION RESULTS:")
            print(f"  âœ… Records with embeddings: {verification['records_with_embeddings']:,}")
            print(f"  âŒ Records without embeddings: {verification['records_without_embeddings']:,}")
            print(f"  ğŸ“ˆ Completion rate: {verification['completion_rate']:.1f}%")
            print(f"  ğŸ“š Chapters processed: {verification['chapters_processed']}")
            
            if verification['sample_embeddings']:
                print(f"\nğŸ“‹ SAMPLE EMBEDDINGS:")
                for i, sample in enumerate(verification['sample_embeddings'][:3], 1):
                    print(f"  {i}. Chapter: {sample['chapter']}")
                    print(f"     Content: {sample['content_preview']}")
                    print(f"     Dimensions: {sample['embedding_dim']}")
                    print(f"     Norm: {sample['embedding_norm']:.3f}")
            
            if verification['completion_rate'] >= 99:
                print(f"\nğŸ‰ MEDICAL EMBEDDING GENERATION SUCCESSFUL!")
                print(f"ğŸ¥ Nelson Pediatrics database is ready for semantic search!")
                print(f"ğŸ” You can now perform AI-powered medical queries!")
            else:
                print(f"\nâš ï¸ PARTIAL COMPLETION")
                print(f"ğŸ”„ {verification['records_without_embeddings']:,} records need reprocessing")

def main():
    """Main execution function"""
    
    print("ğŸ¥ NELSON PEDIATRICS MEDICAL EMBEDDING GENERATION")
    print("=" * 70)
    print("ğŸ¯ Generate medical-optimized embeddings using Hugging Face")
    print("ğŸ” Enable semantic search for pediatric medical content")
    print("=" * 70)
    
    # Initialize generator with medical optimizations
    generator = MedicalEmbeddingGenerator(
        model_name="sentence-transformers/all-mpnet-base-v2",  # High quality model
        batch_size=16,  # Balanced for memory and speed
        max_seq_length=512  # Good for medical text
    )
    
    print(f"\nğŸ¤– CONFIGURATION:")
    print(f"  ğŸ“‹ Model: {generator.model_name}")
    print(f"  ğŸ“¦ Batch size: {generator.batch_size}")
    print(f"  ğŸ“ Max sequence length: {generator.max_seq_length}")
    print(f"  ğŸ¥ Medical text preprocessing: Enabled")
    print(f"  ğŸ” Embedding normalization: Enabled")
    
    # Get user confirmation
    response = input(f"\nâ“ Generate medical embeddings? Type 'GENERATE' to proceed: ").strip()
    
    if response != "GENERATE":
        print("âŒ Medical embedding generation cancelled.")
        return
    
    # Connect to Supabase
    if not generator.connect_to_supabase():
        print("âŒ Cannot connect to Supabase. Check your connection.")
        return
    
    # Load model
    if not generator.load_model():
        print("âŒ Cannot load embedding model.")
        return
    
    # Process embeddings
    success = generator.process_all_embeddings()
    
    # Print summary
    generator.print_summary()
    
    if success:
        print("\n" + "=" * 70)
        print("ğŸ‰ MEDICAL EMBEDDING GENERATION COMPLETE!")
        print("=" * 70)
        print("ğŸ¥ Your Nelson Pediatrics database now supports semantic search")
        print("ğŸ” Ready for AI-powered pediatric medical queries")
        print("ğŸš€ Next: Test semantic search functionality")
    else:
        print("\n" + "=" * 70)
        print("âŒ MEDICAL EMBEDDING GENERATION FAILED!")
        print("=" * 70)
        print("ğŸ” Check the error logs above for details")

if __name__ == "__main__":
    main()

